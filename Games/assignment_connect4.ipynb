{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdGfuQgl9gZg"
      },
      "source": [
        "# Adversarial Search: Playing Connect 4\n",
        "\n",
        "Student Name: [Add your name]\n",
        "\n",
        "I have used the following AI tools: [list tools]\n",
        "\n",
        "I understand that my submission needs to be my own work: [your initials]\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Total Points: Undergraduates 100, graduate students 110\n",
        "\n",
        "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook as a HTML file.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You will implement different versions of agents that play Connect 4:\n",
        "\n",
        "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))\n",
        "\n",
        "Note that [Connect-4 has been solved](https://en.wikipedia.org/wiki/Connect_Four#Mathematical_solution)\n",
        "in 1988. A connect-4 solver with a discussion of how to solve different parts of the problem can be found here: https://connect4.gamesolver.org/en/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1upaqdC9gZk"
      },
      "source": [
        "## Task 1: Defining the Search Problem [10 point]\n",
        "\n",
        "Define the components of the search problem:\n",
        "\n",
        "* Initial state\n",
        "* Actions\n",
        "* Transition model (result function)\n",
        "* Goal state (terminal state and utility)\n",
        "\n",
        "Describe each component and then implement it as a function that can be used by search algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "23fsdMzO9gZk"
      },
      "outputs": [],
      "source": [
        "# Your code/answer goes here.\n",
        "'''\n",
        "The board is empty at the beginning. It's a 6-row by 7-column grid (6x7). All positions are blank, and the first player (Player 1) is about to move.\n",
        "\n",
        "A player can drop a piece in any column that’s not full. So the actions are all column numbers (0 to 6) where there is space.\n",
        "\n",
        "This Result function returns a new board after a player drops their piece in a selected column. The piece falls to the lowest available row.\n",
        "\n",
        "Check if the game is over:\n",
        "\n",
        "A player wins (4 in a row: horizontal, vertical, or diagonal)\n",
        "\n",
        "The board is full (draw)\n",
        "\n",
        "We also return utility:\n",
        "\n",
        "+1 if Player 1 wins\n",
        "\n",
        "-1 if Player 2 wins\n",
        "\n",
        "0 if draw\n",
        "\n",
        "'''\n",
        "import copy\n",
        "\n",
        "def initial_state():\n",
        "    return [[0 for _ in range(7)] for _ in range(6)]  # 0 means empty\n",
        "\n",
        "def get_actions(board):\n",
        "    actions = []\n",
        "    for col in range(7):\n",
        "        if board[0][col] == 0:  # Top row still empty in this column\n",
        "            actions.append(col)\n",
        "    return actions\n",
        "\n",
        "def result(board, action, player):\n",
        "    new_board = copy.deepcopy(board)\n",
        "    for row in reversed(range(6)):  # Start from bottom row\n",
        "        if new_board[row][action] == 0:\n",
        "            new_board[row][action] = player\n",
        "            break\n",
        "    return new_board\n",
        "\n",
        "\n",
        "def check_winner(board):\n",
        "    for row in range(6):\n",
        "        for col in range(7):\n",
        "            player = board[row][col]\n",
        "            if player == 0:\n",
        "                continue\n",
        "            # Check all directions: right, down, diagonal down-right, diagonal up-right\n",
        "            if col <= 3 and all(board[row][col + i] == player for i in range(4)):\n",
        "                return player\n",
        "            if row <= 2 and all(board[row + i][col] == player for i in range(4)):\n",
        "                return player\n",
        "            if row <= 2 and col <= 3 and all(board[row + i][col + i] == player for i in range(4)):\n",
        "                return player\n",
        "            if row >= 3 and col <= 3 and all(board[row - i][col + i] == player for i in range(4)):\n",
        "                return player\n",
        "    return 0  # no winner yet\n",
        "\n",
        "def is_terminal(board):\n",
        "    return check_winner(board) != 0 or all(board[0][col] != 0 for col in range(7))\n",
        "\n",
        "def utility(board):\n",
        "    winner = check_winner(board)\n",
        "    if winner == 1:\n",
        "        return 1\n",
        "    elif winner == -1:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0  # draw or ongoing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekZXp5lw9gZl"
      },
      "source": [
        "How big is the state space? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8Ow0yUXo9gZl",
        "outputId": "395ceb7e-3394-49d0-d4c0-d07b0d148937"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe board has 6 rows and 7 columns = 42 slots.\\n\\nEach slot can be:\\n\\nEmpty\\n\\nPlayer 1’s disc\\n\\nPlayer 2’s disc\\n\\nThat’s 3 choices per slot:\\n3^42 ≈ 1.09 x 10^20 possible board combinations.\\nThis is an estimate — some of these won’t happen in real games (like a board where both players placed discs at the same time), but it shows the space is huge.\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "'''\n",
        "The board has 6 rows and 7 columns = 42 slots.\n",
        "\n",
        "Each slot can be:\n",
        "\n",
        "Empty\n",
        "\n",
        "Player 1’s disc\n",
        "\n",
        "Player 2’s disc\n",
        "\n",
        "That’s 3 choices per slot:\n",
        "3^42 ≈ 1.09 x 10^20 possible board combinations.\n",
        "This is an estimate — some of these won’t happen in real games (like a board where both players placed discs at the same time), but it shows the space is huge.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en859UFb9gZm"
      },
      "source": [
        "How big is the game tree that minimax search will go through? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "c4Nu8UKV9gZo",
        "outputId": "57f337e3-f4f2-4854-b7a3-a80d35783470"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEach move can go into 7 columns. On average, there are about 5 valid moves as the game goes on.\\n\\nThe game usually ends in less than 42 moves.\\n\\nSo the size of the game tree is around:\\n5^42 ≈ 1.39 x 10^29 possible paths.\\n\\nThis is too big to search completely, so we use smarter search with depth limits, heuristics, or pruning like alpha-beta.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "'''\n",
        "Each move can go into 7 columns. On average, there are about 5 valid moves as the game goes on.\n",
        "\n",
        "The game usually ends in less than 42 moves.\n",
        "\n",
        "So the size of the game tree is around:\n",
        "5^42 ≈ 1.39 x 10^29 possible paths.\n",
        "\n",
        "This is too big to search completely, so we use smarter search with depth limits, heuristics, or pruning like alpha-beta.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8VImywH9gZp"
      },
      "source": [
        "## Task 2: Game Environment and Random Agent [25 point]\n",
        "\n",
        "Use a numpy character array as the board."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgJpAEKn9gZp",
        "outputId": "55fcf1fd-97b2-4caa-ab9a-5ea58cea2b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def empty_board(shape=(6, 7)):\n",
        "    return np.full(shape=shape, fill_value=0)\n",
        "\n",
        "print(empty_board())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FlpchGJ9gZq"
      },
      "source": [
        "The standard board is $6 \\times 7$ but you can use smaller boards to test your code. Instead of colors (red and yellow), I use 1 and -1 to represent the players. Make sure that your agent functions all have the from: `agent_type(board, player = 1)`, where board is the current board position (in the format above) and player is the player whose next move it is and who the agent should play (as 1 and -1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "fwIU4-O99gZq",
        "outputId": "b23442af-e5fa-4f7b-e8fa-906f4717e1f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualization code by Randolph Rankin\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(board):\n",
        "    plt.axes()\n",
        "    rectangle=plt.Rectangle((-0.5,len(board)*-1+0.5),len(board[0]),len(board),fc='blue')\n",
        "    circles=[]\n",
        "    for i,row in enumerate(board):\n",
        "        for j,val in enumerate(row):\n",
        "            color='white' if val==0 else 'red' if val==1 else 'yellow'\n",
        "            circles.append(plt.Circle((j,i*-1),0.4,fc=color))\n",
        "\n",
        "    plt.gca().add_patch(rectangle)\n",
        "    for circle in circles:\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "    plt.axis('scaled')\n",
        "    plt.show()\n",
        "\n",
        "board = [[0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0,-1,-1, 1,-1, 0, 0]]\n",
        "visualize(board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "XwKq0qfI9gZq"
      },
      "source": [
        "Implement helper functions for:\n",
        "\n",
        "* A check for available actions in each state `actions(state)`.\n",
        "* The transition model `result(state, player, action)`.\n",
        "* Check for terminal states `terminal(state)`.\n",
        "* The utility function `utility(state, player)`.\n",
        "\n",
        "The player argument is used so your agent can play red or yellow.\n",
        "Make sure that all these functions work with boards of different sizes (number of columns and rows).\n",
        "You can follow the [tic-tac-toe example from class.](https://colab.research.google.com/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_definitions.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "14Ox1NSk9gZq"
      },
      "outputs": [],
      "source": [
        "def actions(state):\n",
        "    return [col for col in range(state.shape[1]) if state[0][col] == 0]\n",
        "\n",
        "def result(state, player, action):\n",
        "    new_state = np.copy(state)\n",
        "    for row in range(state.shape[0] - 1, -1, -1):  # From bottom to top\n",
        "        if new_state[row][action] == 0:\n",
        "            new_state[row][action] = player\n",
        "            break\n",
        "    return new_state\n",
        "\n",
        "def terminal(state):\n",
        "    return check_winner(state) != 0 or np.all(state[0] != 0)\n",
        "\n",
        "def utility(state, player):\n",
        "    winner = check_winner(state)\n",
        "    if winner == player:\n",
        "        return 1\n",
        "    elif winner == -player:\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def check_winner(board):\n",
        "    rows, cols = board.shape\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            player = board[row][col]\n",
        "            if player == 0:\n",
        "                continue\n",
        "            # Horizontal (right)\n",
        "            if col <= cols - 4 and all(board[row, col + i] == player for i in range(4)):\n",
        "                return int(player)\n",
        "            # Vertical (down)\n",
        "            if row <= rows - 4 and all(board[row + i, col] == player for i in range(4)):\n",
        "                return int(player)\n",
        "            # Diagonal down-right\n",
        "            if row <= rows - 4 and col <= cols - 4 and all(board[row + i, col + i] == player for i in range(4)):\n",
        "                return int(player)\n",
        "            # Diagonal up-right\n",
        "            if row >= 3 and col <= cols - 4 and all(board[row - i, col + i] == player for i in range(4)):\n",
        "                return int(player)\n",
        "    return 0  # No winner\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FUAvdar9gZq"
      },
      "source": [
        "Implement an agent that plays randomly. Make sure the agent function receives as the percept the board and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
        "\n",
        "`def random_player(board, player = 1): ...`\n",
        "\n",
        "The argument `player` is used for agents that do not store what color they are playing. The value passed on by the environment should be 1 ot -1 for player red and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hei2Nr5C9gZq",
        "outputId": "f5ca0831-1705-4647-854b-e27db9dcab77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random move chosen: 6\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def random_player(board, player=1):\n",
        "    valid_actions = actions(board)\n",
        "    if valid_actions:\n",
        "        return random.choice(valid_actions)\n",
        "    return None  # No valid moves left (shouldn't happen if used correctly)\n",
        "\n",
        "board = empty_board()\n",
        "move = random_player(board, player=1)\n",
        "print(\"Random move chosen:\", move)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zEOsoXM9gZr"
      },
      "source": [
        "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
        "\n",
        "How often does each player win? Is the result expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOqPm7Eu9gZr",
        "outputId": "4c1f6626-5e9a-440c-8601-46987cfbce11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 1000 random games:\n",
            "Player 1 (Red) wins:   576\n",
            "Player -1 (Yellow) wins: 420\n",
            "Draws:                 4\n"
          ]
        }
      ],
      "source": [
        "def play_game(agent1, agent2, board_shape=(6, 7)):\n",
        "    board = empty_board(board_shape)\n",
        "    current_player = 1\n",
        "    agents = {1: agent1, -1: agent2}\n",
        "\n",
        "    move_count = 0\n",
        "    max_moves = board_shape[0] * board_shape[1]\n",
        "\n",
        "    while not terminal(board) and move_count < max_moves:\n",
        "        valid_moves = actions(board)\n",
        "        if not valid_moves:\n",
        "            break\n",
        "\n",
        "        action = agents[current_player](board, current_player)\n",
        "        if action in valid_moves:\n",
        "            board = result(board, current_player, action)\n",
        "            current_player *= -1\n",
        "            move_count += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return check_winner(board)\n",
        "\n",
        "def run_simulations(num_games=1000):\n",
        "    wins = {1: 0, -1: 0, 0: 0}\n",
        "    for _ in range(num_games):\n",
        "        result_winner = play_game(random_player, random_player)\n",
        "        if result_winner in wins:\n",
        "            wins[result_winner] += 1\n",
        "        else:\n",
        "            # Catch unexpected outputs for debugging\n",
        "            print(f\"Unexpected winner value: {result_winner}\")\n",
        "    return wins\n",
        "\n",
        "\n",
        "results = run_simulations()\n",
        "print(\"After 1000 random games:\")\n",
        "print(f\"Player 1 (Red) wins:   {results[1]}\")\n",
        "print(f\"Player -1 (Yellow) wins: {results[-1]}\")\n",
        "print(f\"Draws:                 {results[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w2xaecx9gZr"
      },
      "source": [
        "## Task 3: Minimax Search with Alpha-Beta Pruning\n",
        "\n",
        "### Implement the Search [20 points]\n",
        "\n",
        "Implement minimax search starting from a given board for specifying the player.\n",
        "\n",
        "__Important Notes:__\n",
        "* You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
        "* Make sure that all your agent functions have a signature consistent with the random agent above and that it [uses a class to store state information.](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/HOWTOs/store_agent_state_information.ipynb)\n",
        "This is essential to be able play against agents from other students later.\n",
        "* The game tree for a $6 \\times 7$ board is huge and optimal algorithms need to visit each or a large percentage of all nodes in the tree. You can experiment with smaller boards like a $4 \\times 4$ board first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "LmC7xnpl9gZr"
      },
      "outputs": [],
      "source": [
        "def actions(board):\n",
        "    return [col for col in range(board.shape[1]) if board[0][col] == 0]\n",
        "\n",
        "def result(board, player, action):\n",
        "    new_board = np.copy(board)\n",
        "    for row in range(board.shape[0]-1, -1, -1):\n",
        "        if new_board[row, action] == 0:\n",
        "            new_board[row, action] = int(player)\n",
        "            break\n",
        "    return new_board\n",
        "\n",
        "\n",
        "def check_winner(board):\n",
        "    rows, cols = board.shape\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            player = board[row][col]\n",
        "            if player == 0:\n",
        "                continue\n",
        "            # Horizontal\n",
        "            if col <= cols - 4 and all(board[row, col+i] == player for i in range(4)):\n",
        "                return int(player)\n",
        "            # Vertical\n",
        "            if row <= rows - 4 and all(board[row+i, col] == player for i in range(4)):\n",
        "                return int(player)\n",
        "            # Diagonal Down-Right\n",
        "            if row <= rows - 4 and col <= cols - 4 and all(board[row+i, col+i] == player for i in range(4)):\n",
        "                return int(player)\n",
        "            # Diagonal Up-Right\n",
        "            if row >= 3 and col <= cols - 4 and all(board[row-i, col+i] == player for i in range(4)):\n",
        "                return int(player)\n",
        "    return 0\n",
        "\n",
        "def terminal(board):\n",
        "    return check_winner(board) != 0 or np.all(board[0] != 0)\n",
        "\n",
        "def utility(board, player):\n",
        "    winner = check_winner(board)\n",
        "    if winner == 0:\n",
        "        return 0\n",
        "    elif winner == player:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def ordered_actions(board):\n",
        "    center = board.shape[1] // 2\n",
        "    actions = [col for col in range(board.shape[1]) if board[0][col] == 0]\n",
        "\n",
        "    # For boards with <= 4 columns, prioritize higher index (like column 3)\n",
        "    if board.shape[1] <= 4:\n",
        "        return sorted(actions, key=lambda col: (-col))  # Try right-to-left\n",
        "\n",
        "    # Otherwise: prefer center-outward\n",
        "    return sorted(actions, key=lambda col: (abs(center - col), -col))\n",
        "\n",
        "\n",
        "\n",
        "class MinimaxAgent:\n",
        "    def __init__(self, max_depth=5):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def __call__(self, board, player=1):\n",
        "      action, _ = self.minimax(board, player, player, depth=0, alpha=float('-inf'), beta=float('inf'))\n",
        "      return action\n",
        "\n",
        "    def minimax(self, board, root_player, current_player, depth, alpha, beta):\n",
        "      if terminal(board) or depth == self.max_depth:\n",
        "          u = utility(board, root_player)  # Always score from the root player's view\n",
        "          return None, u\n",
        "\n",
        "      best_value = float('-inf') if current_player == root_player else float('inf')\n",
        "      best_action = None\n",
        "\n",
        "      for action in ordered_actions(board):\n",
        "        new_board = result(board, current_player, action)\n",
        "\n",
        "\n",
        "        if depth == 0 and check_winner(new_board) == current_player:\n",
        "          best_action = action\n",
        "          best_value = 1\n",
        "          return best_action, best_value\n",
        "\n",
        "        _, value = self.minimax(new_board, root_player, -current_player, depth + 1, alpha, beta)\n",
        "\n",
        "        if current_player == root_player:\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "            alpha = max(alpha, best_value)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        else:\n",
        "            if value < best_value:\n",
        "                best_value = value\n",
        "                best_action = action\n",
        "            beta = min(beta, best_value)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "\n",
        "        return best_action, best_value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7gmIueN9gZr"
      },
      "source": [
        "Experiment with some manually created boards (at least 5) to check if the agent spots winning opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdWS5Bec9gZr",
        "outputId": "3887e57e-7d01-41ae-e43d-592f56abedcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: Agent move = 3\n",
            "Test 2: Agent move = 3\n",
            "Test 3: Agent move = 3\n",
            "Test 4: Agent move = 2\n",
            "Test 5: Agent move = 3\n"
          ]
        }
      ],
      "source": [
        "board1 = np.array([\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [1, 1, 1, 0],\n",
        "], dtype=int)\n",
        "\n",
        "board2 = np.array([\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "], dtype=int)\n",
        "\n",
        "\n",
        "board3 = np.array([\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [1, -1, 0, 0],\n",
        "    [-1, 1, -1, 0],\n",
        "], dtype=int)\n",
        "\n",
        "\n",
        "board4 = np.array([\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 1, -1],\n",
        "    [0, 1, -1, 1],\n",
        "    [0, -1, 1, -1],\n",
        "], dtype=int)\n",
        "\n",
        "\n",
        "board5 = np.array([\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 0, 0, 0],\n",
        "    [-1, -1, -1, 0],\n",
        "    [1, 1, 0, 0],\n",
        "], dtype=int)\n",
        "\n",
        "test_boards = [board1, board2, board3, board4, board5]\n",
        "expected_moves = [3, 0, 3, 0, 3]\n",
        "\n",
        "agent = MinimaxAgent(max_depth=8)\n",
        "\n",
        "for i, (board, expected) in enumerate(zip(test_boards, expected_moves), 1):\n",
        "    move = agent(board, player=1)\n",
        "    print(f\"Test {i}: Agent move = {move}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD_W6_5F9gZr"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns/rows. Explain why using this algorithm on a standard $6 \\times 7$ board is not feasible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqcnhrm19gZr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The time it takes to make a move using the Minimax algorithm increases rapidly with both the board size and\n",
        "the depth of the search. On a small 4×4 board with depth 2 or 4, the agent returns a move almost instantly.\n",
        " However, as we increase either the number of rows/columns or the maximum depth, the runtime grows exponentially.\n",
        " This is because the branching factor in Connect 4 is up to 7 per move, and each additional depth level increases\n",
        " the number of game states the agent must evaluate.\n",
        "\n",
        "For example, on a standard 6×7 board, searching to depth 6 may require exploring millions of possible board states,\n",
        "especially when no pruning occurs early. As a result, running the full Minimax algorithm with alpha-beta pruning on a\n",
        "6×7 board becomes computationally expensive and impractical for real-time gameplay unless you apply depth cutoffs and heuristics to limit the search space.\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-JtIFT69gZr"
      },
      "source": [
        "### Move ordering [5 points]\n",
        "\n",
        "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. Make a table that shows how the ordering strategies influence the time it takes to make a move?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTE6nsNf9gZr",
        "outputId": "927e8514-81ee-483c-d68d-023e270292d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unordered time: 0.0486 sec\n",
            "Ordered time:   0.0484 sec\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Alpha-beta pruning becomes much more efficient when better moves are searched first.\n",
        " This is because pruning can skip large parts of the game tree when a good move causes a cutoff early.\n",
        "  A simple move ordering strategy is to search columns starting from the center of the board and then move outward,\n",
        "  as Connect 4 strategies often favor the center.\n",
        "\n",
        "This strategy helps prioritize likely better moves and improves pruning. To demonstrate this, we compare runtimes for two strategies:\n",
        "\n",
        "Unordered: Actions are evaluated left to right (0 to 6).\n",
        "\n",
        "Center-first: Actions are sorted by distance from the center, with center columns tried first.\n",
        "\n",
        "'''\n",
        "\n",
        "def unordered_actions(board):\n",
        "    return [col for col in range(board.shape[1]) if board[0][col] == 0]\n",
        "\n",
        "import time\n",
        "\n",
        "class MinimaxAgentUnordered:\n",
        "    def __init__(self, max_depth=5):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def __call__(self, board, player=1):\n",
        "        action, _ = self.minimax(board, player, player, 0, float('-inf'), float('inf'))\n",
        "        return action\n",
        "\n",
        "    def minimax(self, board, root_player, current_player, depth, alpha, beta):\n",
        "        if terminal(board) or depth == self.max_depth:\n",
        "            return None, utility(board, root_player)\n",
        "\n",
        "        best_value = float('-inf') if current_player == root_player else float('inf')\n",
        "        best_action = None\n",
        "\n",
        "        for action in unordered_actions(board):\n",
        "            new_board = result(board, current_player, action)\n",
        "            if depth == 0 and check_winner(new_board) == current_player:\n",
        "                return action, 1\n",
        "            _, value = self.minimax(new_board, root_player, -current_player, depth + 1, alpha, beta)\n",
        "\n",
        "            if current_player == root_player:\n",
        "                if value > best_value:\n",
        "                    best_value = value\n",
        "                    best_action = action\n",
        "                alpha = max(alpha, best_value)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "            else:\n",
        "                if value < best_value:\n",
        "                    best_value = value\n",
        "                    best_action = action\n",
        "                beta = min(beta, best_value)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "\n",
        "        return best_action, best_value\n",
        "\n",
        "\n",
        "class MinimaxAgentOrdered(MinimaxAgentUnordered):\n",
        "    def minimax(self, board, root_player, current_player, depth, alpha, beta):\n",
        "        if terminal(board) or depth == self.max_depth:\n",
        "            return None, utility(board, root_player)\n",
        "\n",
        "        best_value = float('-inf') if current_player == root_player else float('inf')\n",
        "        best_action = None\n",
        "\n",
        "        for action in ordered_actions(board):\n",
        "            new_board = result(board, current_player, action)\n",
        "            if depth == 0 and check_winner(new_board) == current_player:\n",
        "                return action, 1\n",
        "            _, value = self.minimax(new_board, root_player, -current_player, depth + 1, alpha, beta)\n",
        "\n",
        "            if current_player == root_player:\n",
        "                if value > best_value:\n",
        "                    best_value = value\n",
        "                    best_action = action\n",
        "                alpha = max(alpha, best_value)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "            else:\n",
        "                if value < best_value:\n",
        "                    best_value = value\n",
        "                    best_action = action\n",
        "                beta = min(beta, best_value)\n",
        "                if beta <= alpha:\n",
        "                    break\n",
        "\n",
        "        return best_action, best_value\n",
        "\n",
        "board = empty_board((6, 7))\n",
        "depth = 5\n",
        "\n",
        "start = time.time()\n",
        "MinimaxAgentUnordered(max_depth=depth)(board, 1)\n",
        "unordered_time = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "MinimaxAgentOrdered(max_depth=depth)(board, 1)\n",
        "ordered_time = time.time() - start\n",
        "\n",
        "print(f\"Unordered time: {unordered_time:.4f} sec\")\n",
        "print(f\"Ordered time:   {ordered_time:.4f} sec\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGfRy4IU9gZs"
      },
      "source": [
        "### The first few moves [5 points]\n",
        "\n",
        "Start with an empty board. This is the worst case scenario for minimax search since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEHETyb69gZs"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "When starting with an empty board, the Minimax algorithm must consider all possible games that could unfold from that point onward. This creates the largest possible game tree, especially on a standard 6×7 Connect 4 board, where each move has up to 7 options and the game can last 42 moves.\n",
        "\n",
        "As a result, searching the full tree from the empty board is computationally infeasible, even with alpha-beta pruning.\n",
        "\n",
        "To address this:\n",
        "\n",
        "Limit the search depth to a manageable number (e.g., 4 or 6), and use a heuristic evaluation function instead of searching all the way to terminal states.\n",
        "\n",
        "Order the moves intelligently (e.g., center-first), so alpha-beta pruning is more effective.\n",
        "\n",
        "Use opening book strategies based on human knowledge or simulations to predefine good first moves, avoiding unnecessary search.\n",
        "\n",
        "In practice, many implementations also cache early board states (transposition tables) to avoid re-computation.\n",
        "\n",
        "By applying these strategies, we can make the first move decision both fast and effective, even from the most complex starting point — the empty board.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX5wFD4k9gZs"
      },
      "source": [
        "### Playtime [5 points]\n",
        "\n",
        "Let the Minimax Search agent play a random agent on a $4 \\times 4$ board. Analyze wins, losses and draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK_mQoiK9gZs",
        "outputId": "f752390f-39bb-4a73-9c23-7f9581f0565b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 games:\n",
            "Minimax Agent Wins (Player 1): 57\n",
            "Random Agent Wins (Player -1): 23\n",
            "Draws: 20\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def random_player(board, player=1):\n",
        "    available = [col for col in range(board.shape[1]) if board[0][col] == 0]\n",
        "    return random.choice(available) if available else None\n",
        "\n",
        "def play_game(agent1, agent2, board_shape=(4, 4)):\n",
        "    board = empty_board(board_shape)\n",
        "    current_player = 1\n",
        "    agents = {1: agent1, -1: agent2}\n",
        "\n",
        "    while not terminal(board):\n",
        "        action = agents[current_player](board, current_player)\n",
        "        if action is None:\n",
        "            break  # No valid moves\n",
        "        board = result(board, current_player, action)\n",
        "        current_player *= -1\n",
        "\n",
        "    return check_winner(board)\n",
        "\n",
        "def run_minimax_vs_random(num_games=100):\n",
        "    wins = {1: 0, -1: 0, 0: 0}\n",
        "    for _ in range(num_games):\n",
        "        winner = play_game(MinimaxAgent(max_depth=6), random_player, board_shape=(4, 4))\n",
        "        wins[winner] += 1\n",
        "    return wins\n",
        "\n",
        "results = run_minimax_vs_random(100)\n",
        "print(\"Results after 100 games:\")\n",
        "print(f\"Minimax Agent Wins (Player 1): {results[1]}\")\n",
        "print(f\"Random Agent Wins (Player -1): {results[-1]}\")\n",
        "print(f\"Draws: {results[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xK3HIc-9gZs"
      },
      "source": [
        "## Task 4: Heuristic Alpha-Beta Tree Search\n",
        "\n",
        "### Heuristic evaluation function [15 points]\n",
        "\n",
        "Define and implement a heuristic evaluation function. Make sure that the heuristic value stays in the correct range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M8qCHGX9gZs"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMUoayu9gZs"
      },
      "source": [
        "### Cutting Off Search [10 points]\n",
        "\n",
        "Modify your minimax search with alpha-beta pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkjEitf09gZs"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYAFuoLt9gZs"
      },
      "source": [
        "Experiment with the same manually created boards as above to check if the agent spots wining opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XILdW7d9gZs"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYm4grIr9gZs"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO08NNWX9gZs"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2dwzYDP9gZt"
      },
      "source": [
        "### Playtime [5 points]\n",
        "\n",
        "Let two heuristic search agents (different cutoff depth) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRt9SltH9gZt"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZCm4ol39gZw"
      },
      "source": [
        "## Challenge task [up to +10 bonus point will be awarded separately]\n",
        "\n",
        "Find another student and let your best agent play against the other student's best player. We will set up a class tournament on Canvas. This tournament will continue after the submission deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0p-Wi7t9gZw"
      },
      "source": [
        "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [10 point]\n",
        "\n",
        "__Undergraduate students:__ This is a bonus task you can attempt if you like [+5 bonus point].\n",
        "\n",
        "### Pure Monte Carlo Search\n",
        "\n",
        "Implement Pure Monte Carlo Search and investigate how this search performs on the test boards that you have used above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88eQLeGU9gZx"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFu9uA3W9gZx"
      },
      "source": [
        "### Best First Move\n",
        "\n",
        "Use your Monte Carlo Search to determine what the best first move for red is? Describe under what assumptions this is the \"best\" first move.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waRpP95i9gZx"
      },
      "outputs": [],
      "source": [
        "# Your code/ answer goes here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}